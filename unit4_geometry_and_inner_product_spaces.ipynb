{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efd41c6b-75db-481e-9598-0f8e7d98ec26",
   "metadata": {},
   "source": [
    "## Unit 4 - Geometry & Inner Product Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f22f7d8-bd40-4e5b-b45f-58f226b3971c",
   "metadata": {},
   "source": [
    "### Vector Geometry\n",
    "\n",
    "Basic Properties of Inner Product Spaces\n",
    "Suppose $V, <\\cdot,\\cdot>$ ($V$ together with an inner product) is an inner product space over $\\mathbb{R}$. Then the following results hold:\n",
    "\n",
    "1) Norm Axioms for $|| \\cdot ||= \\sqrt{<\\cdot,\\cdot>}$ (square root as an inner product of something and itself) ($||\\cdot||$ is the norm function)\n",
    "\n",
    "* $\\forall x,y \\in V, \\forall a \\in \\mathbb{R}$\n",
    "  * $|| x || > 0$ for $x != 0 $\n",
    "  * $|| ax || = |a| \\cdot ||x||$\n",
    "  * $|| x + y || <= ||x|| + ||y||$\n",
    "     \n",
    "2) Cauchy-Schwartz Inequality:\n",
    "\n",
    "* $||<x,y>|| <= ||x|| \\cdot ||y||$\n",
    "\n",
    "3) Parallelogram Law\n",
    "\n",
    "* $ ||x + y||^2 + ||x-y||^2 = 2(||x||^2 + ||y||^2)$\n",
    "* Called the parallelogram law because you can draw a parallelogram between two vectors \n",
    "* Important theoretically to distinguish norms that come from inner products vs. norms that don't come from inner products - the norm comes from an inner product iff it satisfies the parallelogram law\n",
    "\n",
    "4) Pythagorean Theorem (linear algebrea version)\n",
    "* If $<x,y> = 0$ then $||x-y||^2 = ||x||^2 + ||y||^2 = ||x+y||^2$\n",
    "* The inner product of two vectors is 0 if they are at right angles to one another\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae950db-4d9b-4a0a-a96c-6610453269ba",
   "metadata": {},
   "source": [
    "### Inner Product Spaces and Examples\n",
    "\n",
    "An inner product space is a vector space together with an inner product function $< \\cdot, \\cdot>: V \\rightarrow \\mathbb{R}$ that satisfies three axioms. \n",
    "\n",
    "1) Symmetry: $<u,v> = <v,u>$\n",
    "2) Linearity (in the 1st argument): $<u_1+u_2, v> = <u_1,v> + <u_2,v>$ and $<au,v>=a<u,v>$\n",
    "3) Positive Definiteness: $<u,u> >= 0$ and  $<u,u> = 0$ iff $u = \\vec{0}$\n",
    "\n",
    "From any inner product, we can define notions of length, distance and directional correlation (similarity)\n",
    "\n",
    "* length(u) $= \\sqrt{<u,u>}$\n",
    "* distance(u,v) $= \\sqrt{<u-v,u-v>}$\n",
    "* directional_correlation(u,v) $= \\frac{<u,v>}{ \\sqrt{<u,u><v,v>}}$\n",
    "\n",
    "Examples:\n",
    "\n",
    "$\\mathbb{R}^n$ prototypical example\n",
    "* $<x,y> = x^Ty = \\sum_{k=1}^nx_ky_k$\n",
    "* This is the definition used in the first unit to define inner product, matrix vector product, matrix multiplication, etc.\n",
    "\n",
    "We can also define for any diagonal matrix $D$, another inner product on $\\mathbb{R}^n$\n",
    "* $<x,y> = x^TDy = \\sum_{k=1}^nx_kd_{kk}y_k$\n",
    "* In data science, you use this version all the time since this give a notion of distance. Many times in data science, you have pieces of data that don't have the same weight. In a housing data set, when predicting prices, maybe the location feature should be weighed more highly that the other features. This method basically uses linear algebra to apply a diagonal matrix to increase/decrease the weights of the features in the data set. \n",
    "\n",
    "In $\\mathbb{R}[x]$ a typical inner product is\n",
    "\n",
    "* $<p(x), q(x)> = \\int_{-1}^{1}p(x)q(x)dx$\n",
    "* This inner product satisfies the three axioms\n",
    "\n",
    "In $\\mathbb{R}^{n*n}$:\n",
    "\n",
    "* $<A,B> = Trace(A^TB)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ffdaad-d165-411a-8f00-afecc591e807",
   "metadata": {},
   "source": [
    "### Orthonormal Sets of Vectors\n",
    "\n",
    "This section describes a process for taking any basis in an inner product space and converting it to an orthonormal space that has similar geometric properties to the standard basis in $\\mathbb{R}^n$\n",
    "\n",
    "Let $V,<\\cdot,\\cdot>$ be an inner product space. A set of vectors $u_1..u_m$ are called orthogonal if $<u_i, u_j> = 0$ for $i<>j$ (a set of vectors that are pairwaise perpendicular to each other). We say $u_1..u_m$ are orthonormal if $<u_i,u_i> = 1 $ for each $i=1..m$.\n",
    "\n",
    "Example: In $\\mathbb{R}^n$, $e_1,..e_n$ are orthonormal. They are orthonormal because if you take the inner product of any two of them (pairwise with a different subscript), the result is always 0. If you take them pairwise with the same subscript, the result is always 1. \n",
    "\n",
    "Theorem: Given any set of vectors $v_1..v_n$ for an inner product space $V$, there exists a separate set of vectors $u_1..u_n$ such that $u_1..u_n$ are orthonormal and Span($u_1..u_n$) = Span($v_1,v_n$).\n",
    "\n",
    "Proof:\n",
    "\n",
    "Gram-Schmidt Process (Algorithm)\n",
    "\n",
    "```\n",
    "Input v_1..v_n\n",
    "Set w_1 = v_1\n",
    "For k=2...n\n",
    "    Set w_k = v_k - (Sum from j=1 to k ( (<v_j, w_j> / <v_j, v_j>) * v_j ))\n",
    "    # this is the orthogonalization step\n",
    "For k=1...n\n",
    "    Set u_k = w_k / (|| w_k |||)\n",
    "    # this is the orthornormalization step\n",
    "Output u_1...u_n\n",
    "\n",
    "# this will always result in a set of vectors that is orthonormal to the original set\n",
    "\n",
    "```\n",
    "\n",
    "Example: \n",
    "\n",
    "$V = \\mathbb{R}[x]_{<=2}$ (Polynomials with degree <= 2)\n",
    "\n",
    "$<p(x),q(x)> = \\int_{0}^{1}p(x)q(x)dx$\n",
    "\n",
    "Starting basis: {$1,x,x^2$} - this is not orthogonal or orthonormal\n",
    "\n",
    "Orthogonalization step works out to: {$1,x-\\frac{1}{2},x^2-x-\\frac{1}{6}$}\n",
    "\n",
    "Orthonormalization step: {$1, 2\\sqrt{3}(x-\\frac{1}{2}), 6\\sqrt{5}(x^2-x-\\frac{1}{6}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce25476b-76aa-49ac-819a-763bf463fefb",
   "metadata": {},
   "source": [
    "### Adjoints\n",
    "\n",
    "Discuss an inner product space application called an adjoint, a generalization of the notion of a transpose of a linear map between two vector spaces.\n",
    "\n",
    "Let $V, W$ be inner product spaces and let $T \\in L(V,W)$ ($T$ is a linear map from V to W). \n",
    "\n",
    "We want to define a new linear map that's the transpose of $T$. We don't want to define $T$ in terms of its matrix coefficients, we want to define $T$ in terms of its properties.\n",
    "\n",
    "Notice: For every $w \\in W$, the function $<w, \\cdot>: W \\rightarrow \\mathbb{R} \\in L(W,\\mathbb{R})$. The function $<w, \\cdot> is a linear map from $W$ to $\\mathbb{R}$\n",
    "\n",
    "Since $T$ is a function from $V$ to $W$. If we compose $<w, \\cdot>$ with $T$, we get a function (linear map) from $V$ to $\\mathbb{R}$:\n",
    "\n",
    "$<w, \\cdot> \\circ T:V \\rightarrow \\mathbb{R} \\in L(V, \\mathbb{R})$\n",
    "\n",
    "We know, from earlier lessons on linear maps, that every linear map from $V \\rightarrow \\mathbb{R}$ can be represented as an inner product. It has the form:\n",
    "\n",
    "$<v,\\cdot>$ for some $v \\in V$\n",
    "\n",
    "So, we can use these two functions, $<w, \\cdot> \\circ T$ and $<v,\\cdot>$ to define the adjoint of $T$ by \n",
    "\n",
    "$T^*: W \\rightarrow V$\n",
    "\n",
    "$T^*(w)$ = the $v \\in V$ such that $<w, \\cdot> \\circ T = <v, \\cdot>$\n",
    "\n",
    "Another way to say this is: $<w, T(u)> = <v,u> $for every $u \\in V$\n",
    "\n",
    "**Properties of Adjoints**\n",
    "\n",
    "Given any bases  $v_1..v_n$ of $V$ and $w_1..w_n$ of $W$:\n",
    "* $M(T,\\vec{v}, \\vec{w}) = [M(T^*, \\vec{w},\\vec{v})]^{Transpose}$\n",
    "\n",
    "**Special Cases**\n",
    "\n",
    "Definition: A linear map $T \\in L(V)$ is called self-adjoint if $T=T^*$\n",
    "\n",
    "Note: The matrix of a sefl-adjoint linear map is symmetric - the matrix is equal to it's transpose.\n",
    "\n",
    "**Spectral Theorem**\n",
    "\n",
    "If $T \\in L(V)$ is self-adjoint, then $V$ has an orthonormal basis consisting of eigenvectors of $T$.\n",
    "\n",
    "Note: Relative to this basis, $M(T)$ is diagonal (with the eigenvalues on the diagonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f28c53-a0f0-4321-9409-44e3f0506edc",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition\n",
    "\n",
    "One of the most important decompositions that applies to linear maps on inner product spaces. It can be applied to any linear map on an inner product space and relies on adjoints. \n",
    "\n",
    "Let $T \\in L(V)$. $T$ is called positive if $T$ is self-adjoint (its matrix is symmetric) and all of its eigenvalues are positive. \n",
    "\n",
    "Def: For $T$ positive, define $\\sqrt{T}$ to be the unique positive linear map $R \\in L(V)$ such that $R \\circ R = T$\n",
    "\n",
    "Example:\n",
    "\n",
    "$\\sqrt{\\left[\\begin{matrix} 1 & 0 & 0 \\\\ 0 & 4 & 0 \\\\ 0 & 0 & 9 \\end{matrix}\\right]} = \\left[\\begin{matrix} 1 & 0 & 0 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{matrix}\\right]$ \n",
    "\n",
    "This is a positive matrix because all of the eigenvalues are positive\n",
    "\n",
    "Fact: For any $T \\in L(V)$, the linear operator $T^* \\circ T$ is positive. $T^*$ is the adjoint of $T$. Composing $T$ with its adjoint always gives a positive matrix.\n",
    "\n",
    "Definiton: A linear map $S \\in L(V)$ is called an isometry if $||Sv|| = ||v||$ for every $v \\in V$. An isometry is a linear operator that never changes the norms of any vector. It never changes it's distance from the origin. Isometries have many other properties:\n",
    "* Preserves orthonormal bases - It doesn't the fact that the original bases are perpendicular to each other\n",
    "* $S^*S = I = SS^*$ (This means the adjoint of an isometry is also its inverse)\n",
    "\n",
    "Definition: Singular Values of $T$ are the eigenvalues of $\\sqrt{T^*T}$\n",
    "\n",
    "\n",
    "\n",
    "**Single Value Decomposition Theorem**: \n",
    "\n",
    "Every linear map $T \\in L(V)$ can be factored as $T=U \\sum V$ where $U$ and $V$ are isometries and the matrix of $\\sum$ is diagonal with singular values of $T$ on the diagonal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65fb4d1-234e-4bfb-af00-feba6bcc7561",
   "metadata": {},
   "source": [
    "### Orthogonal Projections\n",
    "\n",
    "Applications of inner product spaces to data science\n",
    "\n",
    "Definition: For two vectors $U$, $V$, define the projection of $U$ onto $V$ (denoted $proj_vu$) is the component of $u$ in the direction of $v$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13490fad-bd3b-4e93-ac60-247212a777e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(1, -2, 'proj_v u ')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "m = 1\n",
    "b = 0\n",
    "x = np.linspace(0, 5, 100)\n",
    "y = m*x + b\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# graph spines\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "\n",
    "ax.plot(x, y)\n",
    "ax.text(5, 5, 'u', size=15)\n",
    "\n",
    "x2 = np.linspace(-10, 10, 100)\n",
    "y2 = np.zeros(100)\n",
    "ax.plot(x2, y2)\n",
    "ax.text(10, 0, 'v', size=15)\n",
    "\n",
    "ax.plot([5,5], [5,0])\n",
    "ax.plot([0,5], [0,0] ,linewidth=7.0)\n",
    "\n",
    "ax.text(1, -2, 'proj_v u ', size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd52149-af06-4ea6-8c99-1134e85f0e4c",
   "metadata": {},
   "source": [
    "Definition in terms of inner product spaces:\n",
    "\n",
    "$proj_uv$ = directional_correlation($u,v$) $\\cdot ||u|| \\cdot \\frac{v}{||v||}$\n",
    "\n",
    "directional_correlation $\\cdot ||u||$ is the length. \n",
    "\n",
    "$\\frac{v}{||v||}$ is the direction\n",
    "\n",
    "To compute $proj_uv$, use:\n",
    "\n",
    "$\\frac{<u,v>}{<v,u>}v$ (this is the exact same as what's defined above)\n",
    "\n",
    "**Lemma**: $proj_uv$ is the vector in the same direction as $v$ that is closest to $u$.\n",
    "\n",
    "We want to establish the notion that the projection is the thing that minimizes distance. \n",
    "\n",
    "**Proof Sketch** \n",
    "\n",
    "Define $f(c) = ||cv-u||$ (the distance between $cv$ and $u$)\n",
    "\n",
    "So, if we figure out which value of $c$ minimizes $f(c) = ||cv-u||$, then that will exactly solve the problem and we want to show that $c = \\frac{<u,v>}{<v,u>}$. \n",
    "\n",
    "We want to minimize $f(c)$ which is the same as minimizing \n",
    "\n",
    "$(f(c))^2 = ||cv-u||^2$\n",
    "\n",
    "$(f(c))^2 = <cv-u,cv-u>$\n",
    "\n",
    "$(f(c))^2 = <cv,cv> + <cv,-u> + <-u,cv> + <-u,-u>$\n",
    "\n",
    "$(f(c))^2 = <cv,cv> -2<u,cv> + <u,u>$\n",
    "\n",
    "$(f(c))^2 = c^2<v,v> - 2c<u,v> + <u,u>$\n",
    "\n",
    "$F(c) = (f(c))^2$\n",
    "\n",
    "$F`(c) = 2c<v,v>-2<u,v>$\n",
    "\n",
    "Set $F`(c) = 0 = 2c<v,v>-2<u,v> $\n",
    "\n",
    "$c = \\frac{<u,v>}{<v,u>}$\n",
    "\n",
    "So...\n",
    "\n",
    "$\\frac{<u,v>}{<v,u>}$ is the value of $c$ that minimizes the distance between $cv-u$. The closest vector to $u$ that lies along $v$ is $\\frac{<u,v>}{<v,u>}v$ which is exactly $proj_uv$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f440418b-121b-4f9e-a40d-30aaf545b527",
   "metadata": {},
   "source": [
    "### Least Squares Regression\n",
    "\n",
    "Suppose we have an over-determined (over-constrained) linear system.\n",
    "\n",
    "$ \\left[\\begin{matrix}  A&  &  &  & | b \\\\  &  &  &  & | \\\\  &  &  &  & | \\\\  &  &  &  & | \\\\  &  &  &  & | \\end{matrix}\\right]$\n",
    "\n",
    "$m>n$. There are more rows than columns, typical for a data science problem. More rows than features. \n",
    "\n",
    "We can't solve $Ax = b$ but we can find a vector $x$ that makes $Zx$ as close to $b$ as possible. \n",
    "\n",
    "How to find this $x$?\n",
    "\n",
    "Use orthogonal projections. \n",
    "\n",
    "Suppose $x$ is a solution to the minimization problem $\\min_{x} ||Ax-b||$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4d31e74-76db-4805-be2d-d61027066a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6e9702f5e0>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "m = 1\n",
    "b = 0\n",
    "x = np.linspace(0, 5, 100)\n",
    "y = m*x + b\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# graph spines\n",
    "ax.spines['left'].set_position('zero')\n",
    "ax.spines['right'].set_color('none')\n",
    "ax.spines['bottom'].set_position('zero')\n",
    "ax.spines['top'].set_color('none')\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "\n",
    "ax.plot(x, y)\n",
    "ax.text(5, 5, 'b', size=15)\n",
    "\n",
    "x2 = np.linspace(-10, 10, 100)\n",
    "y2 = np.zeros(100)\n",
    "ax.plot(x2, y2)\n",
    "ax.text(5, 0, 'Ax', size=15)\n",
    "\n",
    "ax.plot([5,5], [5,0])\n",
    "ax.plot([0,5], [0,0] ,linewidth=7.0)\n",
    "\n",
    "#ax.text(1, -2, 'proj_b Ax ', size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9c52e8-efdc-46b6-9ca8-1936f7959b48",
   "metadata": {},
   "source": [
    "Since $x$ is the solution, $Ax$ is just a fixed vector.\n",
    "\n",
    "To say that $Ax$ is the solution is to say that $Ax$ is exactly the vector in the direction of $Ax$ that minimizes from $b$.\n",
    "\n",
    "We get the constraint that $Ax = proj_{Ax}b$\n",
    "\n",
    "$Ax = \\frac{<b,Ax>}{<Ax,Ax>}Ax$ (here, $\\frac{<b,Ax>}{<Ax,b>}$ is just some constant)\n",
    "\n",
    "This implies that $\\frac{<b,Ax>}{<Ax,b>}$ has be to equal to 1 to make the equation true. \n",
    "\n",
    "Therefore, $<b,Ax> = <Ax,Ax>$ (the numerator must equal the denominator)\n",
    "\n",
    "$<b,Ax> = <Ax,Ax>$\n",
    "\n",
    "$(Ax)^Tb = (Ax)^TAx$\n",
    "\n",
    "$x^TA^Tb = x^TA^TAx$\n",
    "\n",
    "This constraint is satisfied if $A^Tb = A^TAx$\n",
    "\n",
    "Theorem: If the columns of $A$ are linearly independent, then $A^TA$ is invertible so that:\n",
    "\n",
    "$A^TAx=A^Tb$ has a unique solution for $x$\n",
    "\n",
    "Any solution for $x$ is also a solution to the linear system:\n",
    "\n",
    "$\\left[\\begin{matrix}  A^TA | A^Tb \\end{matrix}\\right]$\n",
    "\n",
    "This is a relatively easy system to solve with singular value decomposition, eignevalues, etc. using linear algebra instead of vector calculus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67099b59-625f-4e23-b800-012fe8b88823",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
