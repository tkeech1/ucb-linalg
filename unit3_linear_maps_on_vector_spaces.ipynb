{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a602aa6b-21d0-4e05-9114-7bcad2164bde",
   "metadata": {},
   "source": [
    "## Unit 3 - Linear Maps on Vector Spaces\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e0392d-ab0a-408f-a6c9-69e3a57eb0ed",
   "metadata": {},
   "source": [
    "### Vector Spaces\n",
    "\n",
    "Vector spaces are a generalization of the properties of real vectors. Formally, a vector space is a set $V$ together with:\n",
    "* vector addition $+:VxV \\rightarrow V$\n",
    "* scalar multiplication $ \\cdot : \\mathbb{R} x V \\rightarrow V $\n",
    "* additive inverse $-: V \\rightarrow V $\n",
    "* zero vector $ 0 \\in V$\n",
    "\n",
    "That satsfies the following axioms:\n",
    "\n",
    "$\\forall x,y,z \\in V$ and $\\forall a,b \\in \\mathbb{R}$\n",
    "\n",
    "1) $+$ is associative: $x+(y+z) = (x+y)+z$\n",
    "2) $+$ is commutative: $x+y = y+x$\n",
    "3) $0$ is an additive identity: $x+0 = x = 0+x$\n",
    "4) $-$ is an inverse for $+$: $x+(-x) = 0 = -x + x $\n",
    "\n",
    "$<V, + , -, 0>$ is an abelian group\n",
    "\n",
    "Scalar multiplication respects:\n",
    "\n",
    "5) vector addition: $a \\cdot (x+y) = a \\cdot x + a \\cdot y$\n",
    "6) real number addition: $(a + b) \\cdot x = a \\cdot x + b \\cdot x$\n",
    "7) real number multiplication: $(a \\cdot_{\\mathbb{R}} b) \\cdot_{sm} x = a \\cdot_{sm} (b \\cdot_{sm} x)$\n",
    "   NOTE: $a \\cdot b$ is multiplication by real numbers (since $a, b \\in \\mathbb{R}$). Multiplying by $x$ is scalar multiplication since $x \\in V$. On the right hand side, the $a \\cdot b$ is also scalar multiplication.\n",
    "8) $1_{\\mathbb{R}}$: $1_{\\mathbb{R}} \\cdot x = x$ NOTE: without this axiom, you could have a vector space where scalar multiplication trivially assigned everything to zero and this guarntees that doesn't happen. \n",
    "\n",
    "The remainder of the unit relies on these axioms. \n",
    "\n",
    "$\\mathbb{R}^2$ is all possible real-valued 2-tuples - a 2-dimensional real coordinate space\n",
    "$\\mathbb{R}^3$ is all possible real-valued 3-tuples - a 3-dimensional real coordinate space\n",
    "$\\mathbb{R}^n$ is all possible real-valued n-tuples - a n-dimensional real coordinate space\n",
    "\n",
    "Protoypical Example of a Vector Space:\n",
    "\n",
    "$<\\mathbb{R}^n, +_{\\mathbb{R}^n}, \\cdot_{sm}, -_{\\mathbb{R}^n}, 0_{\\mathbb{R}}>$\n",
    "\n",
    "Other Examples of Vector Spaces:\n",
    "\n",
    "* $\\mathbb{R}^{n*m}$ : The set of real $mxn$ matrices is an example of a vector space because you add them, scalar multiply them, you have additive inverses for each of them, and a 0 matrix. Since those operations satisfy the 8 axioms, the set of real $mxn$ matrices is a vector space.\n",
    "* $L(\\mathbb{R}^n \\rightarrow \\mathbb{R}^m)$ : This is linear maps (functions) from $\\mathbb{R}^n to \\mathbb{R}^m$. This is also a vector space because it satisfies the axioms. \n",
    "* $\\mathbb{R}[x]_{<=n}$ : The set of all polynomial functions with real number coefficients where the degree of the polynomial is <= n. \n",
    "* $\\mathbb{R}[x]$ is an example of a vector space that is not finite dimensional. It is the set of all polynomial functions with real number coefficients. It can be arbitrarily long. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c27db8f-1579-4275-917d-0fcd1af60daa",
   "metadata": {},
   "source": [
    "### Linear Combinations\n",
    "\n",
    "**Example from Khan Academy**\n",
    "\n",
    "$v_1, v_2, .. v_n \\in \\mathbb{R}^n$\n",
    "\n",
    "A linear combination of the vectors $v$ means we scale these vectors by constants $c \\in \\mathbb{R}$:\n",
    "\n",
    "$c_1v_1 + c_2v_2 + ... + c_nv_n$\n",
    "\n",
    "Example of a Linear Combination\n",
    "\n",
    "$a = \\left(\\begin{matrix} 1 \\\\ 2 \\end{matrix}\\right)$\n",
    "$b = \\left(\\begin{matrix} 0 \\\\ 3 \\end{matrix}\\right)$\n",
    "\n",
    "$3a + 2b = \\left(\\begin{matrix} 3 \\\\ 0 \\end{matrix}\\right)$ is one linear combination of $a$ and $b$.\n",
    "\n",
    "The set of all linear combinations is the **span**. In this case, Span$(a, b) = \\mathbb{R}^2$ but there are many cases where this isn't true. \n",
    "\n",
    "A case where this is not true is:\n",
    "$a = \\left(\\begin{matrix} 2 \\\\ 2 \\end{matrix}\\right)$\n",
    "$b = \\left(\\begin{matrix} -2 \\\\ -2 \\end{matrix}\\right)$\n",
    "\n",
    "Here, Span($a,b$) falls along a single line because they are colinear. \n",
    "\n",
    "**UCB**\n",
    "\n",
    "Suppose $V$ is a vector space over $\\mathbb{R}$ and $U \\subseteq V$\n",
    "\n",
    "Define \n",
    "\n",
    "$Span(U) = {\\sum_{k=1}^n a_ku_k: a_k \\in \\mathbb{R}, u_k \\in U$}$ (The set of all linear combinations of vectors in $U$)\\\n",
    "\n",
    "We always have $Span(U) \\subseteq V$\n",
    "\n",
    "Definition - We say \"$U$ spans $V$\" if $Span(U) = V$\n",
    "\n",
    "Definition - We say $V$ is finite-dimensional if there exists a finite subset $U$ \\subseteq such that $U$ spans $V$\n",
    "\n",
    "For $U$ to span $V$ there has to be some way to get every element of the vector space by taking linear combination of the elements of $U$.\n",
    "\n",
    "Examples:\n",
    "\n",
    "$\\mathbb{R}^3$ is finite dimensional because the set of vectors:\\\n",
    "$U$ =\n",
    "$\\Bigg\\{\n",
    "\\left(\\begin{matrix} 1 \\\\ 0 \\\\ 0 \\end{matrix}\\right),\n",
    "\\left(\\begin{matrix} 0 \\\\ 1 \\\\ 0 \\end{matrix}\\right),\n",
    "\\left(\\begin{matrix} 0 \\\\ 0 \\\\ 1 \\end{matrix}\\right)\n",
    "\\Bigg\\}\n",
    "$ spans $V$\n",
    "\n",
    "Every vector $v \\in V$ $\\left(\\begin{matrix} a_1 \\\\ a_2 \\\\ a_3 \\end{matrix}\\right)$ can be expressed as: \\\n",
    "$a_1 \\cdot \\left(\\begin{matrix} 1 \\\\ 0 \\\\ 0 \\end{matrix}\\right) + a_2 \\cdot \\left(\\begin{matrix} 0 \\\\ 1 \\\\ 0 \\end{matrix}\\right) + a_3 \\cdot \\left(\\begin{matrix} 0 \\\\ 0 \\\\ 1 \\end{matrix}\\right)$ (Every vector in $V$ can be writte as a linear combination of elements of $U$ so $U$ spans $V$, $U$ spans $\\mathbb{R}^3$, and since a finite set spans $U$, $U$ is finite dimensional.)\n",
    "\n",
    "Also: \n",
    "\n",
    "$\\Bigg\\{\n",
    "\\left(\\begin{matrix} 0 \\\\ 0 \\\\ 0 \\end{matrix}\\right)\n",
    "\\left(\\begin{matrix} 0 \\\\ 0 \\\\ 1 \\end{matrix}\\right)\n",
    "\\left(\\begin{matrix} 0 \\\\ 1 \\\\ 0 \\end{matrix}\\right)\n",
    "\\left(\\begin{matrix} 0 \\\\ 1 \\\\ 1 \\end{matrix}\\right)\n",
    "\\left(\\begin{matrix} 1 \\\\ 0 \\\\ 0 \\end{matrix}\\right)\n",
    "\\left(\\begin{matrix} 1 \\\\ 0 \\\\ 1 \\end{matrix}\\right)\n",
    "\\left(\\begin{matrix} 1 \\\\ 1 \\\\ 0 \\end{matrix}\\right)\n",
    "\\left(\\begin{matrix} 1 \\\\ 1 \\\\ 1 \\end{matrix}\\right)\n",
    "\\Bigg\\}$ spans $V$\n",
    "\n",
    "Example of a not finite dimensional $V$\n",
    "\n",
    "$\\mathbb{R}[x]$ is not finite dimensional ($\\mathbb{R}[x]$ is polynomials in one variable with coefficients in $\\mathbb{R}$) \n",
    "\n",
    "It contains {$1,x^2,x^3,x^4,x^5,...$} which is an infinite set $\\in \\mathbb{R}[x]$) - There's no way to get each of these elements as a linear combination from a finite subset of real polynomials. For example, if you have a finite set of polynomials, you can only have up to the maximum degree of those polynomials. So if the maximum degree is 1000, you can never have $x^1001$ as a linear combination of polynomials whose degree is $<1000$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5f598e-c9f6-4816-b007-1055a49001c8",
   "metadata": {},
   "source": [
    "### Linear Independence and Bases\n",
    "\n",
    "**Khan Academy Example:**\n",
    "\n",
    "The span of $\\Bigg\\{\n",
    "\\left[\\begin{matrix} 2 \\\\ 3 \\end{matrix}\\right],\n",
    "\\left[\\begin{matrix} 4 \\\\ 6 \\end{matrix}\\right]\n",
    "\\Bigg\\}$ is all the points of a single line since [4,6] is a multiple of [2,3]\n",
    "\n",
    "We say they are colinear so their span reduces to a single line. You can't represent everyting in $\\mathbb{R}^2$ with these two vectors. We call this a linearly dependent set. The same goes for any vectors with number of dimensions $\\mathbb{R}^n$\n",
    "\n",
    "**UCB:**\n",
    "\n",
    "Suppose $V$ is a vector space over $\\mathbb{R}$ and that $U \\subseteq V$\n",
    "\n",
    "Define: We say that U is linearly dependent over $\\mathbb{R}$ if there exists nonzero $a_1..a_n \\in \\mathbb{R}$ and nonzero $u_1..u_m \\in U$ with $\\sum_{k=1}^na_ku_k = 0$\n",
    "\n",
    "This means there is some nonzero linear combination of these vectors that sum to 0\n",
    "\n",
    "Define: We say $U$ is linearly independent over $\\mathbb{R}$ if $U$ is not linearly dependent, equivalently:\n",
    "\n",
    "* The usable definition for linearly independent is: Whenever we have $0 = \\sum_{k=1}^na_ku_k = 0$ for nonzero $u_1..u_n \\in U$ this implies a_k = 0 for $k=1..n$\n",
    "\n",
    "**Khan Academy Example:**\n",
    "\n",
    "Is the following set of vectors $U$ linearly dependent or linearly independent?\n",
    "\n",
    "$U$ = $\\Bigg\\{\n",
    "\\left[\\begin{matrix} 2 \\\\ 1 \\end{matrix}\\right],\n",
    "\\left[\\begin{matrix} 3 \\\\ 2 \\end{matrix}\\right]\n",
    "\\Bigg\\}$\n",
    "\n",
    "For $U$ to be linearly dependent, there must be some nonzero $c_1, c_2$ such that\n",
    "\n",
    "$c_1 \\cdot \\left[\\begin{matrix} 2 \\\\ 1 \\end{matrix}\\right] + c_2 \\cdot \\left[\\begin{matrix} 3 \\\\ 2 \\end{matrix}\\right] = 0$\n",
    "\n",
    "If the only way to satisfy the above equation is to set $c_1, c_2$ to zero, then they are linearly independent. \n",
    "\n",
    "$2c_1 + 3c_2 = 0$\\\n",
    "$c_1 + 2c_2 = 0$\n",
    "\n",
    "$c_1 + 3/2c_2 = 0$ (multiply top equation by $1/2$)\\\n",
    "$c_1 + 2c_2 = 0$\n",
    "\n",
    "$-1/2c_2 = 0$ (subtract the bottom equation from the top)\\\n",
    "$c_2 = 0$\n",
    "\n",
    "$c_1 + 2(0) = 0 $ (substitute $c_2$ into the original equation)\\\n",
    "$c_1 = 0$\n",
    "\n",
    "$U$ is linearly independent because the only solution to this equation requires $c_1, c_2$ to be zero. This also means that Span($U$) $= \\mathbb{R}^2$\n",
    "\n",
    "#### Bases\n",
    "Define: $U$ is a basis for $V$ if:\n",
    "1) $U$ spans $V$\n",
    "2) $U$ is linearly indepndent over $\\mathbb{R}$\n",
    "\n",
    "#### What are we saying with all this?\n",
    "* Linear Combinations are a way to combine $n$ vectors to produce another vector\n",
    "* Spans define reachability criteria and address the question - Can we get all elements in the vector space?\n",
    "* Linear independence says that there's only one way to get the 0 vector by taking a linear combination of the set.  \n",
    "* If we have a set that meets both span and linear independence, it's a bases.\n",
    "* A basis is the minimum set of vectors that spans the vector space or subspace (There can be no redundancy (linear dependence) within a basis)\n",
    "\n",
    "Examples:\n",
    "\n",
    "**Example of linearly dependent**\n",
    "\n",
    "$\\Bigg\\{\n",
    "\\left(\\begin{matrix} 1 \\\\ 1 \\end{matrix}\\right),\n",
    "\\left(\\begin{matrix} 0 \\\\ 1 \\end{matrix}\\right),\n",
    "\\left(\\begin{matrix} 1 \\\\ 0 \\end{matrix}\\right)\n",
    "\\Bigg\\} = U \\subseteq \\mathbb{R}^2$ is linearly dependent. For this to be true, we need to be able to write the 0 vector as a combination of these three vectors using non-zero coefficients. One way to do that is:\n",
    "\n",
    "$1 \\cdot \\left(\\begin{matrix} 1 \\\\ 1 \\end{matrix}\\right) + (-1) \\cdot \\left(\\begin{matrix} 0 \\\\ 1 \\end{matrix}\\right) + (-1) \\cdot \\left(\\begin{matrix} 1 \\\\ 0 \\end{matrix}\\right)  = 0 $\n",
    "\n",
    "**Example of linearly independent**\n",
    "\n",
    "$\\Bigg\\{\n",
    "\\left(\\begin{matrix} 1 \\\\ 1 \\end{matrix}\\right),\n",
    "\\left(\\begin{matrix} 0 \\\\ 1 \\end{matrix}\\right)\n",
    "\\Bigg\\} = U \\subseteq \\mathbb{R}^2$ is linearly independent. How can we show that?\n",
    "\n",
    "Suppose we have a linear combination of these and it's equal to zero\n",
    "\n",
    "Suppose $a_1 \\cdot \\left(\\begin{matrix} 1 \\\\ 1 \\end{matrix}\\right) + a_2 \\cdot \\left(\\begin{matrix} 0 \\\\ 1 \\end{matrix}\\right) = \\left(\\begin{matrix} a_1 \\\\ a_1 + a_2 \\end{matrix}\\right) = \\left(\\begin{matrix} 0 \\\\ 0 \\end{matrix}\\right)$\n",
    "\n",
    "We can solve this small linear system and find that $a_1 = 0$ and $a_2 = 0$. This meets the definition of linearly independent because the definition states that any time we get 0 as a linear combination of vectors, then the only way for that to happen is for the coefficients to be zero. \n",
    "\n",
    "$\\Bigg\\{\n",
    "\\left(\\begin{matrix} 1 \\\\ 1 \\end{matrix}\\right),\n",
    "\\left(\\begin{matrix} 0 \\\\ 1 \\end{matrix}\\right)\n",
    "\\Bigg\\} = U \\subseteq \\mathbb{R}^2$ is a bases for $\\mathbb{R}^2$\n",
    "\n",
    "To show that, let $\\left(\\begin{matrix} a_1 \\\\ a_2 \\end{matrix}\\right) \\in \\mathbb{R}^2$ and now we need to show we can get $\\left(\\begin{matrix} a_1 \\\\ a_2 \\end{matrix}\\right)$ as a linear combination of $\\Bigg\\{\n",
    "\\left(\\begin{matrix} 1 \\\\ 1 \\end{matrix}\\right),\n",
    "\\left(\\begin{matrix} 0 \\\\ 1 \\end{matrix}\\right)\n",
    "\\Bigg\\}$ \n",
    "\n",
    "$a_1 \\cdot \\left(\\begin{matrix} 1 \\\\ 1 \\end{matrix}\\right) + (a_2 - a_1) \\cdot \\left(\\begin{matrix} 0 \\\\ 1 \\end{matrix}\\right) = \\left(\\begin{matrix} a_1 \\\\ a_2 \\end{matrix}\\right)$ - This shows that any vector in $\\mathbb{R}^2$ can be written as a linear combination of the vectors $[1,1]$ and $[0,1]$ therefore this set spans the whole vector space and is also a bases. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abd4b78-b0bc-479f-9893-16458ba081b7",
   "metadata": {},
   "source": [
    "### The Dimension Theorem\n",
    "\n",
    "A fundamental result that says that the cardinality of any two bases of a vector space is the same. \n",
    "\n",
    "Theorem - Any two bases of a vector space have the same cardinality.\n",
    "\n",
    "Definition (based on the theorem) - The dimension of a vector space is the unique cardinality of any basis. (This says that bases for a given vector space must have the same number of elements - you can't have one base that has 2 elements and another that has 3)\n",
    "\n",
    "Example - The dimension of $\\mathbb{R}^n$ is $n$ since {$e_1, e_2,... e_n$} is a basis which has $n$ elements. So, every basis of $\\mathbb{R}^n$ has $n$ elements. {$e_1, e_2,... e_n$} is the diagonal matrix of 1's from unit 1 (i.e. the basis vector i-hat and j-hat for $\\mathbb{R}^2$). \n",
    "\n",
    "**Proof Sketch:**\n",
    "\n",
    "Lemma: If $T$ is linearly independent over $\\mathbb{R}$ and $S$ spans $V$ and $T, S \\subseteq V$ the $|T| <= |S|$ (cardinality of T <= cardinality of S)\n",
    "\n",
    "Enumerate $T = t_1, t_2, t_3, ...$ (not assuming this is finite)\n",
    "\n",
    "Since $S$ spans $V$, we can write:\n",
    "\n",
    "$t_1 = \\sum_{k=1}^ma_ks_k$ with each $s_k \\in S$ and each $a_k != 0 \\in \\mathbb{R}$\n",
    "\n",
    "Solve for $s_1$ \n",
    "\n",
    "$s_1 = \\frac{t_1 - \\sum_{k=1}^ma_ks_k}{a_1}$\n",
    "\n",
    "Bucket analogy - an injective function from $T$ into $S$ - For each $t_i$ I can always find an $s_i in S$ but not in $T$ such that when I write $t_i = \\sum_{k=1}^na_ku_k$ with $u_k \\in S'$ then there is some $s_i \\in S$ but not in T that I can pop out of the bucket.\n",
    "\n",
    "TODO - ? Explain this better\n",
    "\n",
    "Now, we need to prove the dimension theorem from the above Lemma. Suppose $B_1, B_2$ are both bases. The Lemma says that the Linearly Independent set can never be bigger than the Spanning set. \n",
    "\n",
    "We can think of this in 2 ways:\n",
    "1) $B_1$ is linearly independent and $B_2$ spans $V$ which implies $|B_1 <= |B_2|$\n",
    "2) $B_2$ is linearly independent and $B_1$ spans $V$ which implies $|B_2 <= |B_1|$\n",
    "\n",
    "**The Main Idea** - For a given vector space, the number of elements in a basis is unique and is an invariant of that vector space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf042fd-e8e0-4903-be0c-c51ebd47e2c2",
   "metadata": {},
   "source": [
    "### Representations of Linear Maps\n",
    "\n",
    "We will take a linear map of a vector space and represent it as a matrix. Representations of linear maps of vector spaces depend on a basis. We also had to derive the dimension theorem so we would know that the size of our matrix is fixed. \n",
    "\n",
    "Suppose $U, V$ are vector spaces over $\\mathbb{R}$\n",
    "\n",
    "Definition: A linear map (aka. function) from $U$ to $V$ is a function $T: U \\rightarrow V$ satisfying the linear condition for all $u_1, u_2 \\in U$ and $a \\in \\mathbb{R}$:\n",
    "* $T(u_1 + u_2) = T(u_1) + T(u_2)$\n",
    "* $T(au_1)  = aT(u_1)$\n",
    "\n",
    "The set of all linear maps from $U$ into $V$ is denoted by $L(U,V)$\n",
    "\n",
    "In Unit 1, we saw that if $U = \\mathbb{R}^n$ and $W = \\mathbb{R}^m$ then every linear map in $L(\\mathbb{R}^n,\\mathbb{R}^m)$ can be represented uniquely as a matrix $A \\in \\mathbb{R}^{m*n}$\n",
    "\n",
    "* For $\\mathbb{R}^n$, we have a natural notion of a basis ($e_1, e_2, e_3, ... e_n$) which have nice geometric properties but we can't quite use those properties yet. For now, we stick with addition and scalar multiplication.\n",
    "\n",
    "Theorem:\n",
    "\n",
    "If $u_1, u_2, ... u_n$ is a basis for $U$ and $w_1, w_2,... w_n$ is a basis for $W$ and $T \\in L(U, W)$ ($T$ is a linear map from $U$ into $W$) then $T$ has a matrix representation in the bases $u_1, u_2, ... u_n$ $w_1, w_2,... w_n$ denoted $M(T, (u_1, u_2, ... u_n), (w_1, w_2,... w_n)) \\in \\mathbb{R}^{m*n}$\n",
    "\n",
    "* This is analagous to the situation from Unit 1 where the vector spaces were $\\mathbb{R}^n$ and $\\mathbb{R}^m$ and we got an $\\mathbb{R}^{m*n}$ matrix\n",
    "\n",
    "The coefficients are $[a_{ij}]_{i=1..m,j=1...n}$ where $T(u_j) = a_{ij}w_i$\n",
    "* This is slightly different than the definition for real matrices we covered in Unit 1: $a_{ij} = <T(u_j),w_i>$ \n",
    "* The Unit 1 definition is not defined yet because we haven't defined inner product on an arbitrary vector space\n",
    "\n",
    "**The Main Idea** - If you have any linear map (function) between two vector spaces and the vector spaces are finite dimensional then, with respect to any fixed bases of those vector spaces, there is a matrix representation of that linear map. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c64404-0e68-4ed4-85d1-4b7d7a014d9f",
   "metadata": {},
   "source": [
    "#### Differentiation as a Linear Map\n",
    "\n",
    "We will construe the calculus differentiation operator as a linear map and represent it as a matrix to show an application of the representation theorem for linear maps\n",
    "\n",
    "Let $V = \\mathbb{R}[x]_{<=3}$ = The set of all polynomials in one variable with coefficients in $\\mathbb{R}$ with degree $<= 3$ (no exponents larger than 3). It can be represented by the set:\n",
    "* {$a_0 + a_1x + a_2x^2 + a_3x^3 : a_0, a_1, a_2, a_3 \\in \\mathbb{R} $}\n",
    "* This is a vector space because:\n",
    "  * It contains the zero vector\n",
    "  * The result of addition lies within the space (additive inverse)\n",
    "  * The result of scalar multiplication lies within the space (scalar multiplication)\n",
    "  \n",
    "What is an example of a basis for this vector space?\n",
    "\n",
    "Claim: {$1,x,x^2,x^3$} is a basis for $V$. Why?\n",
    "\n",
    "Need to show 2 things:\n",
    "1) That the set spans $V$ (i.e. every polynomial can be written as a linear combination of the elements). This is obvious by the definition. We can get any degree <= 3 polynomial by multiplying each term by its coefficient.\n",
    "2) That the elements are linearly independent. For any linear combination of these elements to equal 0, the only way for that to happen is by setting all coefficients equal to 0. That's also true in this case because if any of the coefficients are non-zero, you won't get the zero polynomial. \n",
    "\n",
    "Let $W = \\mathbb{R}[x]_{<=2}$\n",
    "\n",
    "Define $T:V \\rightarrow W$ by $T(P(x)) = \\frac{d}{dx}P(x)$\n",
    "* $T$ is the derivative of $P(x)$\n",
    "\n",
    "I claim $T \\in L(V, W)$ ($T$ is a linear function from $V \\rightarrow W$) so $T$ must preserve addition and scalar multiplication. We need to check to see if that's true\n",
    "\n",
    "* Addition\n",
    "  * $T(P(x) + Q(x)) = \\frac{d}{dx}(P(x) + Q(x)) = \\frac{d}{dx}P(x) + \\frac{d}{dx}Q(x) = T(P(x)) + T(Q(x))$ (since the derivative operator respects sums)\n",
    "* Scalar Multiplication\n",
    "  * $T(aP(x)) = \\frac{d}{dx}aP(x) = a \\frac{d}{dx}P(x) = aT(P(x))$ (since the derivative respects multiplication)\n",
    "\n",
    "So, $T$ is a linear map. \n",
    "\n",
    "What is the matrix of $T$ in the bases {$1, x, x^2, x^3$} = {$v_1, v_2, v_3, v_4$} of $V$ and {$1, x, x^2$} = {$w_1, w_2, w_3$} of $W$?\n",
    "\n",
    "Use the definition of matrix construction: $T(v_j) = \\sum_{i=1}^ma_{ij}w_i$ (so we will write out what $T$ does to a basis for $v$ in terms of the basis for $w$ and from that set of equations, we can read off the coefficients of the matrix)\n",
    "\n",
    "$T(v_1) = T(1) = \\frac{d}{dx}1 = 0$ (since the derivative of 1 is 0)\n",
    "* There's only one way to write 0 as the linear combination of $a_{ij}w_i$: set all $a_{ij}$ elements to 0. \n",
    "\n",
    "$T(v_2) = T(x) = \\frac{d}{dx}x = 1$ = $w_1$ (so $a_{ij} = 1$)\n",
    "\n",
    "$T(v_3) = T(x^2) = \\frac{d}{dx}x^2 = 2x = 2w_2$ (so $a_{ij} = 2$)\n",
    "\n",
    "$T(v_4) = T(x^3) = \\frac{d}{dx}x^3 = 3x^2 = 3w_3$ (so $a_{ij} = 3$)\n",
    "\n",
    "Since $T: V \\rightarrow W$, $T$ will be a 3x4 matrix. $T(v_1)$ is going to give us the first column of the matrix. $T(v_2)$ gives the second column and so on.\n",
    "\n",
    "$M(T) = \\left[\\begin{matrix} 0&1&0&0 \\\\ 0&0&2&0 \\\\ 0&0&0&3 \\\\ \\end{matrix}\\right]$\n",
    "\n",
    "This is the matrix of the differentiation operator. Let's check this actually works.\n",
    "\n",
    "$P(x) = 7 + 5x - 2x^2 + x^3$\n",
    "\n",
    "Instead of taking the derivative using calculus, we can use linear algebra and the matrix we just created. We represent the polynomial above as a vector of coefficients. \n",
    "\n",
    "$\\left[\\begin{matrix} 0&1&0&0 \\\\ 0&0&2&0 \\\\ 0&0&0&3 \\\\ \\end{matrix}\\right]\n",
    "* \n",
    "\\left(\\begin{matrix} 7 \\\\ 5 \\\\ -2 \\\\ 1 \\end{matrix}\\right)\n",
    "= \n",
    "\\left(\\begin{matrix} 5 \\\\ -4 \\\\ 3 \\end{matrix}\\right)\n",
    "$\n",
    "\n",
    "$\\frac{d}{dx}P(x) = 5 - 4x + 3x^2$\n",
    "\n",
    "Another Example\n",
    "\n",
    "$S: W \\rightarrow V$ by $S(q(x)) = \\int q(x)dx$\n",
    "\n",
    "\\* is indeterminate in the matrix below\n",
    "\n",
    "$M(S) = \\left[\\begin{matrix} *&*&* \\\\ 1&0&0 \\\\ 0&\\frac{1}{2}&0 \\\\ 0&0&\\frac{1}{3} \\end{matrix}\\right]$\n",
    "\n",
    "Note: $T \\circ S = I_w = \\left[\\begin{matrix} 1&0&0 \\\\ 0&1&0 \\\\ 0&0&1 \\end{matrix}\\right]$\n",
    "\n",
    "The composition of $T$ and $S$ is the identity matrix of $W$. This is the Fundamental Theorem of Calculus (If you first integrate, then differentiate, you get back the original result).\n",
    "\n",
    "$S \\circ T = \\left[\\begin{matrix} 0&*&*&* \\\\ 0&1&0&0 \\\\ 0&0&1&0 \\\\ 0&0&0&1 \\end{matrix}\\right]$\n",
    "\n",
    "If you start with a function, take it's derivative and then take its anti-derivative, you don't get back the original function because of the indeterminants. You lose the constant terms. \n",
    "\n",
    "**The Main Idea** - This example shows how you can use linear algebra to do basic calculus operations and it shows how to represent a linear map (function) as a matrix with respect to a basis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9a0742-af50-45ab-80bc-c5f80bc8f46a",
   "metadata": {},
   "source": [
    "### Subspaces and Direct Sums\n",
    "\n",
    "This section focuses on how to select a basis such that the linear map with respect to that basis has a nice representation\n",
    "\n",
    "Let $V$ be a vector space and let $U \\subseteq V$\n",
    "\n",
    "Definition: We say $U$ is a subspace of $V$ if $U$ is a vector space itself with all of the operations ($+, \\cdot_{sm}, -, 0$) restricted to $U$\n",
    "* Subspaces need to be closed under addition, scalar multiplication, and must contain the zero vector\n",
    "* A span is a good way to get a subspace because it is closed under all linear combinations\n",
    "\n",
    "Example: \n",
    "\n",
    "$V = \\mathbb{R}^3$\n",
    "\n",
    "$U =$ Span {$\\left(\\begin{matrix} 1 \\\\ 0 \\\\ 0 \\end{matrix}\\right),\\left(\\begin{matrix} 0 \\\\ 1 \\\\ 0 \\end{matrix}\\right)$}\n",
    "\n",
    "$U$ is a subspace of $V$ because it is closed under addition and scalar multiplication for all linear combinations\n",
    "\n",
    "Sum of Vector Spaces:\n",
    "\n",
    "Let $U_1, U_2, .. U_n$ be subspaces of $V$\n",
    "\n",
    "Define Sum of Vector Spaces:\n",
    "\n",
    "$\\sum_{k=1}^n U_k = $ { $\\sum_{k=1}^n a_ku_k$ where each $a_k \\in \\mathbb{R}$ and each $u_k \\in U_k$}\n",
    "* The sum is the set of all linear combinations pairwise in each of the individual $U$s\n",
    "\n",
    "Example:\n",
    "\n",
    "$V = \\mathbb{R}[x]$ (the set of all polynomials with real coefficients)\n",
    "\n",
    "For $k=1,..10$ let $U_k = \\mathbb{R}[x]_{<=k}$\n",
    "\n",
    "What is $\\sum_{k=1}^{10} U_k$?\n",
    "\n",
    "Claim $\\sum_{k=1}^{10} U_k = \\mathbb{R}[x]_{<=10}$\n",
    "\n",
    "Now we need to get to the notion of direct sum. To do that, we need to talk about linear independence of subspaces.\n",
    "\n",
    "Definition of Linearly Independent Subspaces: \n",
    "\n",
    "We say subspaces $U_1,...U_n$ are linearly independent if the only way to write $\\sum_{k=1}^n a_ku_k = 0$ with $a_k \\in \\mathbb{R}$ and each $u_k$ nonzero vector in $U_k$ is to take $a_k = 0$ for $k=1...n$\n",
    "\n",
    "Definition of Direct Sum:\n",
    "\n",
    "If $U_1..U_n$ are linearly independent, we call the sum a direct sum and we replace $\\sum$ with $\\bigoplus$. We only use $\\bigoplus$ when the subspaces being summed are linearly independent.\n",
    "\n",
    "Example:\n",
    "\n",
    "$V = \\mathbb{R}^3$\n",
    "\n",
    "$U_1$ = Span($\\left(\\begin{matrix} 1 \\\\ 0 \\\\ 0 \\end{matrix}\\right),\\left(\\begin{matrix} 0 \\\\ 1 \\\\ 0 \\end{matrix}\\right)$)\n",
    "\n",
    "$U_2$ = Span($\\left(\\begin{matrix} 0 \\\\ 0 \\\\ 1 \\end{matrix}\\right)$)\n",
    "\n",
    "$V = U_1 \\bigoplus U_2$\n",
    "\n",
    "Or we can also write in summation notation as:\n",
    "\n",
    "$V = \\bigoplus_{k=1}^2 U_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948cc29f-66b5-44f6-b629-2aadbb8ce7bd",
   "metadata": {},
   "source": [
    "### Invariant Subspaces of Linear Maps\n",
    "\n",
    "Finding nice bases for linear maps such that they have a simple matrix.\n",
    "\n",
    "Let $V$ be a vector space and let $T \\in L(V)$\n",
    "\n",
    "Definition: We say that a subspace $W$ of $V$ is invariant under $T$ if $T(w) \\in W$ for every $w \\in W$ \n",
    "\n",
    "This means that if we look at a function $T$ and restrict it to the domain $W$, we get a function from $W \\rightarrow V$. And, a subspace is invariant under $T$ if when we apply $T$ to any element from $W$, we get another element of $W$.\n",
    "\n",
    "Examples\n",
    "\n",
    "* {0} and $V$ are invariant subspaces of every $T \\in V$\n",
    "* Kernel($T$) = { $v \\in V : T(v) = 0 $} is invariant under $T$\n",
    "* Range($T$) = {$Tv : v \\in V$} is invariant under $T$\n",
    "\n",
    "A Non-trivial Example:\n",
    "\n",
    "$T \\in L(\\mathbb{R}^2)$\n",
    "\n",
    "$T \\left(\\begin{matrix} x_1 \\\\ x_2 \\end{matrix}\\right) = \\left(\\begin{matrix} 2x_1 \\\\ 3x_2 \\end{matrix}\\right)$\n",
    "\n",
    "Then Span $\\left(\\begin{matrix} 1 \\\\ 0 \\end{matrix}\\right)$ and Span $\\left(\\begin{matrix} 0 \\\\ 1 \\end{matrix}\\right)$ are both invariant under $T$. \n",
    "\n",
    "A Nice Property of a Matrix\n",
    "\n",
    "Suppose $V = U \\bigoplus W$ and $U$ and $W$ are both invariant under $T$. Let ($u_1..u_m$) be a basis for $U$ and let ($w_1...w_p$) be a basis for $W$. Then\n",
    "\n",
    "$M(T, (u_1..u_m, w_1..w_p))$ has the form:\n",
    "\n",
    "$\\left[\\begin{matrix} M(T|_{u^{u_1..u_m}})&0 \\\\ 0&M(T|_{w^{w_1..w_p}}) \\end{matrix}\\right]$\n",
    "\n",
    "The upper diagonal contain the matrix of $T$ restricted to $U$ and the lower diagonal is the matrix of $T$ restricted to $W$. This is what you get when you have a direct sum of invariant subspaces of a linear map. You get a matrix that can be decomposed into 2 blocks. The matrix of $T$ is just the linear map on $U$ and the linear map on $W$ in a diagonal matrix. \n",
    "\n",
    "Goal - We want to find subspaces $U_1...U_m$ of $V$ such that:\n",
    "1) $V = \\bigoplus_{k=1}^2 U_k$\n",
    "2) Each $U_k$ is invariant under $T$\n",
    "\n",
    "A linear subspace has a representation as a matrix $T$ with respect to a basis. We need to find a simple basis so we have find many subspaces $U$ that are as small as possible such that the matrix representation of $T$ is as simple as possible. If you use the wrong basis, $T$ can be huge and complicated.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59546f1a-3bb3-4b7c-a68c-a52d5d1d1fe0",
   "metadata": {},
   "source": [
    "### Eigenvalues and Eigenvectors\n",
    "\n",
    "From Khan Academy:\n",
    "\n",
    "We want to find basis vectors that will give us simple transformation matrices. One way to find these basis vectors is to find eigenvectors and eigenvalues. Eigenvectors are vectors that are simply stretched by some scaling factor (their direction is not changed by the tranformation) when the transformation matrix is applied to them. Eigenvalues are the scaling factors by which the eigenvectors get stretched. \n",
    "\n",
    "Think about an example transformation that creates a mirror image of a vector around some line. Any vector on the \"mirror\" line is an eigenvector. An eigenvalue is any scaling factor that extends the line in one direction or another. Another eigenvector is the vector perpendicular to the line. One of it's eigenvalues is -1 since this simply flips the vector across the \"mirror\" line. Both of these vectors are probably good basis vectors.  \n",
    "\n",
    "UCB:\n",
    "\n",
    "We want to decompose a vector space into a direct sum of subspaces that are invariant under a linear map. The reason we want to do this is because if we represent a linear map in a basis, then the matrix is simpler and can be represented as a block diagonal matrix.\n",
    "\n",
    "Eigenvectors will give us subspaces of $V$ that are invariant under $T$ that have dimension 1. The smaller the subspace that's invariant under $T$, the better since gives a simpler representation.\n",
    "\n",
    "Let $V$ be a vector space and $T \\in L(V)$ ($T$ is a linear map from $V \\rightarrow V$)\n",
    "\n",
    "Definition: If there exists $\\lambda \\in \\mathbb{R}$ and $ u \\in V$ with $Tu = \\lambda u$ then we say $\\lambda $ is an eigenvalue of $T$ and $u$ is an eigenvector of $T$ corresponding to $\\lambda$\n",
    "* Note: The Span($u$) is a subspace of $V$ that is invariant under $T$. Why? $u$ being an eigenvector means that if we apply $T$ to $u$, it's just stretching $u$ because $Tu = \\lambda u$ since this just says we multiply $u$ by some real constant. The same is true for Span($u$) since $T$ is linear so $Tcu = \\lambda cu$ (multiplying $u$ by some constant $c$ still just stretches $u$). Every element in the Span($u$) is also an eigenvector (since $u$ is an eigenvector) and the Span($u$) is an invariant subspace of $V$ under $T$. \n",
    "\n",
    "Example:\n",
    "\n",
    "$M(T) = \\left[\\begin{matrix} 1&0&0 \\\\ 0&2&0 \\\\ 0&0&3 \\\\ \\end{matrix}\\right]$ then the eigenvalues of $T$ are 1,2,3 and some eigenvectors of $T$ are $\\left(\\begin{matrix} 1 \\\\ 0 \\\\ 0 \\end{matrix}\\right), \\left(\\begin{matrix} 0 \\\\ 1 \\\\ 0 \\end{matrix}\\right), \\left(\\begin{matrix} 0 \\\\ 0 \\\\ 1 \\end{matrix}\\right)$\n",
    "\n",
    "* In general, it's not true that the diagonal values of the matrix are the eigenvalues as shown in the example above. \n",
    "\n",
    "Theorem: If $M(T)$ is either upper triangular or lower triangular, then all the eigenvalues of $T$ are on the diagonal of $M(T)$\n",
    "\n",
    "* If you have an upper or lower triangular matrix, you can read-off the eigenvalues from the diagonal of the matrix and that's typically the easiest way to find the eigenvalues - you do some transformation so that you find a basis relative to which the basis has an upper or lower triangular matrix and the eigenvalues will be on the diagonal\n",
    "\n",
    "**Generalized Eigenvectors**\n",
    "\n",
    "Suppose $\\lambda$ is an eigenvalue of $T$. The follwing three things all mean the same thing:\n",
    "* $T(u) = \\lambda u $\n",
    "* $(T-\\lambda I)u = 0$ ($I$ is the identity matrix)\n",
    "* $u \\in kernel(T-\\lambda I)$\n",
    "\n",
    "$w$ is a generalized eigenvector corresponding to $\\lambda$ if $w \\in kernel(T- \\lambda I)^n$ where $n$ is the dimenion of the vector space.\n",
    "\n",
    "Example:\n",
    "\n",
    "$\\left[\\begin{matrix} 0&1 \\\\ 0&0 \\end{matrix}\\right]$\n",
    "\n",
    "The only eigenvalue is 0. The Span $\\left(\\begin{matrix} 1 \\\\ 0 \\end{matrix}\\right)$ contains all the eigenvectors. \n",
    "\n",
    "Check that $\\left(\\begin{matrix} 0 \\\\ 1 \\end{matrix}\\right)$ is a generalized eigenvector .\n",
    "\n",
    "$\\left[\\begin{matrix} 0&1 \\\\ 0&0 \\end{matrix}\\right]\n",
    "\\left[\\begin{matrix} 0&1 \\\\ 0&0 \\end{matrix}\\right]\n",
    "\\left(\\begin{matrix} 0 \\\\ 1 \\end{matrix}\\right)$\n",
    "\n",
    "$\\left[\\begin{matrix} 0&0 \\\\ 0&0 \\end{matrix}\\right]\n",
    "\\left(\\begin{matrix} 0 \\\\ 1 \\end{matrix}\\right)$ (we got this by multiplying the two matrices together)\n",
    "\n",
    "$\\left(\\begin{matrix} 0 \\\\ 0 \\end{matrix}\\right)$ (we got this by multiplying the zero matrix by the vector)\n",
    "\n",
    "This means that $\\left(\\begin{matrix} 0 \\\\ 1 \\end{matrix}\\right)$ is a generalized eigenvector corresponding to the only eignevalue which is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61262b5a-0ae6-46d2-8109-e403cb4c7822",
   "metadata": {},
   "source": [
    "### Characteristic Polynomials\n",
    "\n",
    "The characteristic polynomial gives information about what the linear map does. In some cases it's determinant but in other cases it's not. \n",
    "\n",
    "Let's let $V$ be a vector space over \\mathbb{R} and let $T \\in L(V)$:\n",
    "\n",
    "Simple Case:\n",
    "\n",
    "* $\\exists$ (There exists) a basis of $V$ for which $M(T, \\bar{b})$ (the matrix of T with respect to the basis) is upper triangular\n",
    "* $M(T,\\bar{b}) = \\left[\\begin{matrix} d_{11} & x & x & x & x \\\\ 0 & d_{22} & x & x & x  \\\\ 0 & 0 & d_{33} & x & x \\\\\n",
    "0 & 0 & 0 & d_{44} & x \\\\ 0 & 0 & 0 & 0 & d_{nn} \\end{matrix}\\right]$ the the eigenvalues of $T$ are $d_{11}...d_{nn}$ and the characteristic polynomial of $T$ is $\\prod_{k=1}^n(x-d_{kk})$\n",
    "* The roots of the polynomial are exactly the eigenvalues \n",
    "* This is the case that occurs most of the time but sometimes it doesn't work\n",
    "\n",
    "Complex Case:\n",
    "* In general, the eigenvalues of $T$ may not be real numbers\n",
    "* Characteristic polynomial is a product of 2 types of smaller polynomials of the forms:\n",
    "  * ($x-\\lambda_k$): This is the form that's used anytime you have a real eigenvalue\n",
    "  * $(x^2 + \\alpha_jx + \\beta_j)$: This is the form that's used when you have complex eigenvalues in conjugate pairs. The product of those pairs will give a quadratic polynomial) \n",
    "* Define: For each $k$, $U_k =$ kernel$((T-\\lambda_kI)^{dimensionV})$\n",
    "* Define: For each $j$, $W_j =$ kernel$( (T^2 + \\alpha_jT + \\beta_jI)^{dimensionV})$\n",
    "* Each $U_k, W_j$ is invariant under $T$\n",
    "* $V = \\bigoplus_k U_k + \\bigoplus_jW_j$ (V is the direct sum of the U_ks and W_js) \n",
    "* The characteristic polynomial of $T$ is $\\prod_{k}^n(x-\\lambda_k)^{dimensionU_k} \\cdot \\prod_j(x^2+\\alpha_jx+\\beta_j)^{dimensionW_j}$ \n",
    "\n",
    "In general, we'll only use the case where the polynomial's roots are the diagonal of the matrix. \n",
    "\n",
    "Cayley Hamilton Theorem\n",
    "\n",
    "If $P(x)$ is the characteristic polynomial of linear map $T$, then the polynomial applied to the linear map is 0: $P(T) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0926fe7-1d3b-41c7-a9cf-dc95067f93ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Trace and Determinant\n",
    "\n",
    "Let $V$ be a vector space over $\\mathbb{R}$. Let $T \\in L(V)$\n",
    "\n",
    "Suppose $P(x) = \\sum_{k=1}^na_kx^k$ is the characteristic polynomial of $T$. $P$ is a polynomial of degree $n$.\n",
    "\n",
    "Define: Trace($T$) = $-a_{n-1}$\n",
    "* $a_{n-1}$ is the coefficient of the $n-1$ term\n",
    "\n",
    "Define: Det($T$)$ = (-1)^na_0$\n",
    "* $a_0$ is the constant term in the polynomial\n",
    "\n",
    "Suppose Dim($V$) = 1 so $M(T) = [a]$ for some $a \\in \\mathbb{R}$. \n",
    "\n",
    "Then, the characteristic polynomial of $T$ is $x-a$ (it just has one eigenvalue $a$ which we plug into $(x-\\lambda_k)$ from earlier).\n",
    "* Trace($T$) = a \n",
    "  * Since $-a_{n-1} = a$ for n=1\n",
    "* Determinant($T$) = a \n",
    "  * Since $(-1)^n -a = a$ for n=1\n",
    "  \n",
    "Suppose $T$ has $n$ eigenvalues $\\lambda_1...\\lambda_n$\n",
    "\n",
    "Then the characteristic polynomial of $T$ is $\\prod_{k}^n(x-\\lambda_k)^{dimensionU_k}$ \n",
    "\n",
    "Define: Trace($T$) = $\\sum_{k=1}^n\\lambda_k$ (sum of the eigenvalues)\n",
    "\n",
    "Define: Det($T$) = $\\prod_{k=1}^n\\lambda_k$ (product of the eigenvalues)\n",
    "\n",
    "**Another Property of Trace and Determinant**\n",
    "\n",
    "If $V$ = $\\bigoplus_{k=1}^n U_k$ where each $U_k$ is invariant under $T$\n",
    "* Trace($T$) = $\\sum_{k=1}^m$Trace($T$|$U_k$) (| means restricted to $U_k$)\n",
    "* Determinant($T$) = $\\prod_{k=1}^m$Trace($T$|$U_k$) \n",
    "\n",
    "**Nice Properties of Trace**\n",
    "* Trace($S+T$) = Trace($S$) + Trace($T$)\n",
    "* For any basis $v_1..v_n$ of $V$, Trace($T$) = sum of diagonal elements of $M(T,\\vec{v})$\n",
    "  * This is a cool way to compute Trace that makes it easy to find for any matrix. Regardless of the basis you choose, Trace is just the sum of the diagonals. The matrix doesn't have to be in diagonal form. And, regardless of the basis you choose, the sum of the diagonals is always the same. \n",
    "* Trace($T \\circ S$) = Trace($S \\circ T$) (Trace of T composed with S is the same as Trace S composed with T)\n",
    "\n",
    "**Nice Properties of Determinant**\n",
    "* Det($S \\circ T$) = Det($T \\circ S$) = Det($T$) $\\cdot$ Det($S$) (you can unfactor the determinants and take their matrices separately)\n",
    "* You can compute the characteristic polynomial in terms of the determinant\n",
    "  * Characteristic polynomial of $T$ = Det($xI-T$)\n",
    "  * The determinant is a function that takes linear operators and turns them into functions\n",
    "* If $V = \\mathbb{R}^n$ and $x \\subseteq V$\n",
    "  * In $\\mathbb{R}^2$, $x$ is some blob that has an area\n",
    "  * In $\\mathbb{R}^{>=3}$, $x$ is some blob that has a volume\n",
    "  * Volume($T(x)$) = | Det($T$) | $\\cdot$ Volume($x$)\n",
    "    * If we have a way to get the volume of $x$, we can use the determinant to calculate the volume of $x$ once the linear map $T$ has been applied to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c6356e-195e-480e-9aff-5b63fbc390bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
